{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(Sequence):\n\n  def __init__(self, csv_file, base_dir, output_size, shuffle=False, batch_size=10):\n    \"\"\"\n    Initializes a data generator object\n      :param csv_file: file in which image names and numeric labels are stored\n      :param base_dir: the directory in which all images are stored\n      :param output_size: image output size after preprocessing\n      :param shuffle: shuffle the data after each epoch\n      :param batch_size: The size of each batch returned by __getitem__\n    \"\"\"\n    self.df = pd.read_csv(csv_file)\n    self.base_dir = base_dir\n    self.output_size = output_size\n    self.shuffle = shuffle\n    self.batch_size = batch_size\n    self.on_epoch_end()\n\n  def on_epoch_end(self):\n    self.indices = np.arange(len(self.df))\n    if self.shuffle:\n      np.random.shuffle(self.indices)\n\n  def __len__(self):\n    return int(len(self.df) / self.batch_size)\n\n  def __getitem__(self, idx):\n    ## Initializing Batch\n    #  that one in the shape is just for a one channel images\n    # if you want to use colored images you might want to set that to 3\n    X = np.empty((self.batch_size, *self.output_size, 1))\n    # (x, y, h, w)\n    y = np.empty((self.batch_size, 4, 1))\n\n    # get the indices of the requested batch\n    indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n\n    for i, data_index in enumerate(indices):\n      img_path = os.path.join(self.base_dir,\n                  self.df.iloc[data_index, 0])\n\n      img = mpimg.imread()\n      img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) # to reduce it to one channel to match the shape\n      ## this is where you preprocess the image\n      ## make sure to resize it to be self.output_size\n\n      label = self.df.iloc[data_index, 1:].to_numpy()\n      ## if you have any preprocessing for\n      ## the labels too do it here\n\n      X[i,] = img\n      y[i] = label\n\n    return X, y\n\n\n## Defining and training the model\n\nmodel = Sequential([\n  ## define the model's architecture\n])\n\ntrain_gen = DataGenerator(\"data.csv\", \"data\", (244, 244), batch_size=20, shuffle=True)\n\n## compile the model first of course\n\n# now let's train the model\nmodel.fit(train_gen, epochs=5, ...)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define Features and Label\n#features = ['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID', 'pe_present_on_image', 'negative_exam_for_pe',\n           # 'qa_motion', 'flow_artifact', 'rv_lv_ratio_gte_1', 'leftsided_pe', 'chronic_pe', 'true_filling_defect_not_pe', 'rightsided_pe', \n           # 'acute_and_chronic_pe', 'central_pe', 'indeterminate'] \n\n#x=train[features].values\n#y=train[features].values\n\n#Considering y variable holds numpy array\n#y_tensor = tf.convert_to_tensor(y, dtype=tf.int64) \n\n\n#Train Test Split\n#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport vtk\nimport cv2\nimport time\nimport pydicom\nimport numpy as np \nimport pandas as pd \nimport scipy.ndimage\nimport seaborn as sns\n\n\nfrom glob import glob\nfrom skimage import measure\nfrom tensorflow import keras\nfrom plotly import __version__\nfrom plotly.graph_objs import*\nfrom skimage import morphology\nfrom vtk.util import numpy_support\nfrom sklearn.cluster import KMeans\nfrom skimage.transform import resize\nfrom IPython.display import clear_output\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom plotly.tools import FigureFactory as FF\nfrom tensorflow.keras.models import load_model\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\nfrom tensorflow.keras.callbacks import ModelCheckpoint as MC\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Conv2D\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\n#import tensor as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tensorflow.keras.models import Sequential, Model,load_model\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.initializers import glorot_uniform\nimport numpy as np\nnp.random.seed(1000)\n\n \nmodel=Sequential()\ninput_shape=(224, 224, 3)\nmodel.add(Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), input_shape=input_shape, padding='same')) #1st convo\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same')) #2nd convo\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same')) #3rd convo\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same')) #4th convo\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same')) #5th convo\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Flatten())\nmodel.add(Dense(4096, input_shape=(224*224*3,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Dense(4096))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Dense(17))\nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\n\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's visualize layer names and layer indices to see how many layers\n# we should freeze:\nfrom tensorflow.keras import layers\nfor i, layer in enumerate(model.layers):\n   print(i, layer.name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we chose to train the top 2 conv blocks, i.e. we will freeze\n# the first 8 layers and unfreeze the rest:\nprint(\"Freezed layers:\")\nfor i, layer in enumerate(model.layers[:20]):\n    print(i, layer.name)\n    layer.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainable parameters decrease after freezing some bottom layers   \nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image preprocessing\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 128\n#base_dir = \"../input/rsna-str-pulmonary-embolism-detection\"\nroot_path ='../input/rsna-str-pulmonary-embolism-detection'\n\nx = train_datagen.flow_from_directory(root_path+'/train',\n                                                 target_size=(224, 224),\n                                                 batch_size=batch_size,\n                                                 class_mode='categorical')\n\ny = test_datagen.flow_from_directory(root_path+'/test',\n                                            target_size=(224, 224),\n                                            batch_size=batch_size,\n                                            class_mode='categorical')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_dict = training_set.class_indices\nprint(class_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"li = list(class_dict.keys())\nprint(li)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_num = x.samples\ntest_num = y.samples","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the dataset for training and testing.\ndef is_test(x, _):\n    return x % 4 == 0\n\n\ndef is_train(x, y):\n    return not is_test(x, y)\n\n\nrecover = lambda x, y: y\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import Sequence \nfrom skimage.io import imread\nfrom skimage.transform import resize\nimport numpy as np\nimport math\n# Here, `x_set` is list of path to the images\n# and `y_set` are the associated classes.\nclass  rsna_str_pulmonary_embolism_detectionSequence(Sequence):\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n        self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n        self.batch_size]\n        return np.array([\n            resize(imread( rsna_str_pulmonary_embolism_detection), (200, 200))for rsna_str_pulmonary_embolism_detection in batch_x]), np.array(batch_y)\n\nx =  rsna_str_pulmonary_embolism_detectionSequence((),(),100)\n\nfrom tensorflow.python.keras.engine import data_adapter\nadapter = data_adapter.KerasSequenceAdapter(x,y=None)\nprint(adapter)\nmodel.fit(x, y, batch_size=10, epochs=5, verbose=1, shuffle=True)\n\nscore=model.evaluate(x, y)\nprint('Test Loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nx_train=np.asarray(x_train).astype(np.int)\ny_train=np.asarray(y_train).astype(np.int)\n\nmodel.fit(x_train, y_train, batch_size=10, epochs=5, verbose=1, validation_split=0.2, shuffle=True)\n\n\nscore=model.evaluate(x_train, y_train)\nprint('Test Loss:', score[0])\nprint('Test accuracy:', score[1]\n      model.save('pe_detection_model.h0')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}